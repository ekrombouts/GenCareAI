{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekrombouts/GenCareAI/blob/main/scripts/100_note_generation/130_RAGIndexing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_63aAoRXj07"
      },
      "source": [
        "# GenCare AI: Retrieval Augmented Generation of care notes\n",
        "\n",
        "**Author:** Eva Rombouts  \n",
        "**Date:** 2024-06-03  \n",
        "**Updated:** 2024-07-01  \n",
        "**Version:** 1.2\n",
        "\n",
        "### Description\n",
        "In this script the anonymous client notes generated [here](https://github.com/ekrombouts/GenCareAI/blob/main/scripts/100_GenerateAnonymousCareNotes.ipynb) are processed and stored in a vector database (Chroma). It enables querying of the database using OpenAI's embeddings and a retrieval-augmented generation system.\n",
        "\n",
        "*Document Loading and Processing*: Documents are loaded from the Hugging Face platform, split into smaller sections by a LangChain Text Splitter, and pre-processed. The notes are split into smaller chunks, even though this probably was not necessary for this dataset. This step was taken for completeness to ensure scalability in the future.  \n",
        "*Database Initialization and Population*: A Chroma vector database is initialized and populated it with the embedded document chunks.  \n",
        "*Query Operations*: The RetrievalQA pipeline let's us search the database using natural language queries, demonstrating the capability to retrieve and display relevant information.\n",
        "\n",
        "### Goal\n",
        "My goal is to use this vector database to retrieve relevant examples for few-shot inference in prompts for creating synthetic client notes. This approach can help me improve the generation of this data by providing specific, contextually relevant examples that guide the model's results.\n",
        "\n",
        "### Setup and configuration\n",
        "- When running in CoLab Google Drive is mounted to persistently store the Chroma vector database.\n",
        "- Retrieve API keys for OpenAI and HuggingFace, providing authentication for accessing the [embedding model](https://platform.openai.com/docs/guides/embeddings), the [QA model](https://platform.openai.com/docs/models) and the [dataset](https://huggingface.co/datasets/ekrombouts/dutch_nursing_home_reports). \n",
        "\n",
        "### Recommended Resources\n",
        "- [RAG - Retrieval Augmented Generation](https://www.youtube.com/playlist?list=PL8motc6AQftn-X1HkaGG9KjmKtWImCKJS) with Sam Witteveen on YouTube\n",
        "- [Python RAG Tutorial (with Local LLMs)](https://www.youtube.com/watch?v=2TJxpyO3ei4&t=323s) by Pixegami on YouTube\n",
        "- And of course the [Langchain documentation](https://python.langchain.com/v0.1/docs/use_cases/question_answering/)\n",
        "\n",
        "***Please note*** that the embedding isn't free. Embedding the 35.000+ notes costs appr $0.15. The costs for the examples of querying the database in this notebook are negligible.\n",
        "\n",
        "Version 1.2: Term 'reports' changed to 'notes'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# Determines the current environment (Google Colab or local)\n",
        "def check_environment():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return \"Google Colab\"\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    return \"Local Environment\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installs and settings depending on the environment\n",
        "# When running in CoLab, the Google drive is mounted and necessary packages are installed.\n",
        "# Data paths are set and API keys retrieved\n",
        "\n",
        "env = check_environment()\n",
        "\n",
        "if env == \"Google Colab\":\n",
        "    print(\"Running in Google Colab\")\n",
        "    !pip install -q langchain langchain-openai langchain-community chromadb datasets\n",
        "    from google.colab import drive, userdata\n",
        "    drive.mount('/content/drive')\n",
        "    DATA_DIR = '/content/drive/My Drive/Colab Notebooks/GenCareAI/data'\n",
        "    OPENAI_API_KEY = userdata.get('GCI_OPENAI_API_KEY')\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "else:\n",
        "    print(\"Running in Local Environment\")\n",
        "    # !pip install -q chromadb datasets\n",
        "    DATA_DIR = '../data'\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "    OPENAI_API_KEY = os.getenv('GCI_OPENAI_API_KEY')\n",
        "    HF_TOKEN = os.getenv('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxPS7wYjrugf"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules from Langchain and Hugging Face\n",
        "from datasets import load_dataset\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
        "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.chains import RetrievalQA\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCGSQTBMrzxT"
      },
      "outputs": [],
      "source": [
        "# Constants for dataset and storage paths\n",
        "PATH_HF_DATASET = 'ekrombouts/dutch_nursing_home_notes'\n",
        "PATH_DB_GCAI = os.path.join(DATA_DIR, 'chroma_db_gcai_notes')\n",
        "COLLECTION_NAME = 'anonymous_notes'\n",
        "MODEL = 'text-embedding-ada-002'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tILbjl3zuQI8"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l29SPKZNsfuN"
      },
      "outputs": [],
      "source": [
        "def load_documents():\n",
        "  \"\"\"Load the dataset from Hugging Face, ensuring access via token.\"\"\"\n",
        "  dataset = load_dataset(PATH_HF_DATASET, token=HF_TOKEN)\n",
        "  loader = HuggingFaceDatasetLoader(PATH_HF_DATASET,\n",
        "                                    page_content_column='note')\n",
        "  return loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCRJwVFXuyE9"
      },
      "outputs": [],
      "source": [
        "def split_documents(documents: list[Document]):\n",
        "  \"\"\"Split large text documents into manageable chunks for better handling by ML models.\"\"\"\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size=800,\n",
        "      chunk_overlap=100)\n",
        "  chunks = text_splitter.split_documents(documents)\n",
        "  # Index each chunk to maintain unique identifiers\n",
        "  for idx, chunk in enumerate(chunks):\n",
        "      chunk.metadata['id'] = str(idx)\n",
        "  return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZKYyBXp7iTp"
      },
      "outputs": [],
      "source": [
        "def initialize_vectordb(persist_directory, embedding_function, collection_name):\n",
        "    \"\"\"Initialize the Chroma vector database, either loading an existing one or creating a new one.\"\"\"\n",
        "    if os.path.exists(persist_directory):\n",
        "        return Chroma(persist_directory=persist_directory,\n",
        "                      embedding_function=embedding_function,\n",
        "                      collection_name=collection_name)\n",
        "    else:\n",
        "        return Chroma.from_documents(documents=[],\n",
        "                                     embedding=embedding_function,\n",
        "                                     persist_directory=persist_directory,\n",
        "                                     collection_name=collection_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wild1x4IpGYa"
      },
      "outputs": [],
      "source": [
        "def load_existing_ids(vectordb):\n",
        "    \"\"\"Fetch existing document IDs from the database to avoid duplicates.\"\"\"\n",
        "    try:\n",
        "        existing_items = vectordb.get(include=[])\n",
        "        existing_ids = set(existing_items[\"ids\"])\n",
        "    except:\n",
        "        existing_ids = set()\n",
        "    return existing_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Psz5WVXpope"
      },
      "outputs": [],
      "source": [
        "# Modified from https://github.com/pixegami/rag-tutorial-v2/blob/main/populate_database.py\n",
        "def add_new_documents(vectordb, documents):\n",
        "    \"\"\"Add new documents to the database only if they don't already exist.\"\"\"\n",
        "    existing_ids = load_existing_ids(vectordb)\n",
        "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
        "    # Only add documents that don't exist in the DB.\n",
        "    new_documents = []\n",
        "    for document in documents:\n",
        "        if document.metadata[\"id\"] not in existing_ids:\n",
        "            new_documents.append(document)\n",
        "    if len(new_documents):\n",
        "        print(f\"Adding new documents: {len(new_documents)}\")\n",
        "        new_document_ids = [document.metadata[\"id\"] for document in new_documents]\n",
        "        vectordb.add_documents(new_documents, ids=new_document_ids)\n",
        "    else:\n",
        "        print(\"No new documents to add\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK31hG7c1T6o"
      },
      "source": [
        "### Embed and store texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REruMCsLyNoL"
      },
      "outputs": [],
      "source": [
        "# Load, split and process documents\n",
        "documents = load_documents()\n",
        "chunks = split_documents(documents=documents)\n",
        "\n",
        "# # Consider experimenting with a smaller dataset\n",
        "# # For large datasets you might need to split it in two\n",
        "# documents_sample = documents[:20000]\n",
        "# chunks = split_documents(documents=documents_sample)\n",
        "\n",
        "print(len(documents))\n",
        "print(len(chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MFHbqkV9eIz"
      },
      "outputs": [],
      "source": [
        "# Initialize vector databse, using OpenAI embeddings\n",
        "embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=MODEL)\n",
        "vectordb = initialize_vectordb(PATH_DB_GCAI, embedding, COLLECTION_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrk3uLoB93g4"
      },
      "outputs": [],
      "source": [
        "# Add new documents to the database\n",
        "add_new_documents(vectordb, chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56otpzQ509Lu"
      },
      "source": [
        "### Query the database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuGkHJ26AhVs"
      },
      "outputs": [],
      "source": [
        "## To read the db from file\n",
        "\n",
        "# vectordb = Chroma(persist_directory=FN_DB_GCAI,\n",
        "#                   embedding_function=OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=MODEL),\n",
        "#                   collection_name = COLLECTION_NAME\n",
        "#                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVUpJYDt2eNQ"
      },
      "outputs": [],
      "source": [
        "# Delete all items in the db\n",
        "\n",
        "# items = vectordb.get(include=[])\n",
        "# existing_ids = items[\"ids\"]\n",
        "# vectordb.delete(ids=existing_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOkFn9g_-XSZ"
      },
      "outputs": [],
      "source": [
        "# Retrieve metadata and document IDs from the database\n",
        "items = vectordb.get(include=['metadatas'])\n",
        "existing_ids = set(items[\"ids\"])\n",
        "metadata = items['metadatas']\n",
        "print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
        "print(metadata[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEQfxXps_D0x"
      },
      "outputs": [],
      "source": [
        "# Set up a retriever for document querying\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOLn6rsX-k7m"
      },
      "outputs": [],
      "source": [
        "# Query the vector database using similarity search\n",
        "query = 'gewichtsverlies'\n",
        "docs = retriever.invoke(query)\n",
        "\n",
        "print(f'Number of docs: {len(docs)}\\n')\n",
        "print(f'Retriever search type: {retriever.search_type}\\n')\n",
        "\n",
        "print(f'Documents most similar to \"{query}\":')\n",
        "for doc in docs:\n",
        "  print(doc.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfgNejX3_WrV"
      },
      "outputs": [],
      "source": [
        "# Initialize the QA chain for answering questions using the retrieved documents\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(api_key=OPENAI_API_KEY),\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrJeQA1Y5Rec"
      },
      "outputs": [],
      "source": [
        "# Define a function to process the response\n",
        "def process_llm_response(llm_response):\n",
        "    print(100 * '*')\n",
        "    print(f\"\\nresult: {llm_response['result']}\")\n",
        "    print('\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['id'], source.metadata['topic'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUHXH3SYCYbI"
      },
      "outputs": [],
      "source": [
        "# function to test the pipeline\n",
        "def query_retrieval_pipeline(query):\n",
        "  llm_response = qa_chain.invoke(query)\n",
        "  pprint(llm_response)\n",
        "  print(process_llm_response(llm_response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GD6uq1UzS8ng"
      },
      "outputs": [],
      "source": [
        "query_retrieval_pipeline (\"Wat moet je doen als je client afvalt in gewicht?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFZ8vW4-C-LW"
      },
      "outputs": [],
      "source": [
        "query_retrieval_pipeline(\"Wat moet je doen als je client agressief gedrag vertoont?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SJAb2aXDg0P"
      },
      "outputs": [],
      "source": [
        "query_retrieval_pipeline(\"Wat kan je doen als een cliÃ«nt onrustig is 's nachts?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_V8JtBEEZeA"
      },
      "outputs": [],
      "source": [
        "query_retrieval_pipeline(\"Welke interventies zijn ingezet voor het verbeteren van de nachtrust?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54V-v_euVl5r"
      },
      "outputs": [],
      "source": [
        "query_retrieval_pipeline(\"Wat zijn leuke dingen om te doen met bezoek?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwShUZWgn6Sx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNgBOefXHBMTWJpNUOwkdE5",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
