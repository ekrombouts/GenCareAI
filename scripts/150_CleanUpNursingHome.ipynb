{"cells":[{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://colab.research.google.com/github/ekrombouts/GenCareAI/blob/main/scripts/150_CleanUpNursingHome.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"ipdYvH-RuXl7"},"source":["# GenCare AI: Concat and clean data\n","\n","**Author:** Eva Rombouts  \n","**Date:** 2024-07-11  \n","**Version:** 1.0\n","\n","### Description\n","This script concatenates the profiles, scenario's, client records and summaries generated in different experiments (each stored as different 'wards') and restructures and cleans up the data.\n","\n","The result is a set of 4 datasets:\n","1. Galaxy_clients\n","2. Galaxy_scenarios\n","3. Galaxy_records\n","4. Galaxy_summaries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhtBxmkAncAM"},"outputs":[],"source":["import os\n","# Determines the current environment (Google Colab or local)\n","def check_environment():\n","    try:\n","        import google.colab\n","        return \"Google Colab\"\n","    except ImportError:\n","        pass\n","\n","    return \"Local Environment\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2qPYAobxneZ_"},"outputs":[],"source":["# Installs and settings depending on the environment\n","# When running in CoLab, the Google drive is mounted and necessary packages are installed.\n","# Data paths are set and API keys retrieved\n","\n","env = check_environment()\n","\n","if env == \"Google Colab\":\n","    print(\"Running in Google Colab\")\n","    !pip install datasets -q\n","    from google.colab import drive, userdata\n","    drive.mount('/content/drive')\n","    DATA_DIR = '/content/drive/My Drive/Colab Notebooks/GenCareAI/data'\n","else:\n","    print(\"Running in Local Environment\")\n","    # !pip install\n","    DATA_DIR = '../data'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nx_U5CkEmsRR"},"outputs":[],"source":["import pandas as pd\n","import ast\n","\n","from datasets import Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WFr58E4qm92u"},"outputs":[],"source":["CARE_HOME = 'Galaxy'\n","wards = ['Horizon', 'Venus', 'Tulip', 'Cosmos']\n","\n","FN_CLIENTS_DF = os.path.join(DATA_DIR, f'gcai_{CARE_HOME}_clients.csv')\n","FN_SCENARIOS_DF = os.path.join(DATA_DIR, f'gcai_{CARE_HOME}_scenarios.csv')\n","FN_RECORDS_DF = os.path.join(DATA_DIR, f'gcai_{CARE_HOME}_records.csv')\n","FN_SUMMARIES_DF = os.path.join(DATA_DIR, f'gcai_{CARE_HOME}_summaries.csv')\n","\n","hf_repo_name = \"ekrombouts/\" + CARE_HOME"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def concatenate_files(data_dir, wards, file_prefix):\n","    df_list = []\n","    for ward in wards:\n","        file_name = os.path.join(data_dir, f'{file_prefix}_{ward}.csv')\n","        if os.path.exists(file_name):\n","            df = pd.read_csv(file_name)\n","            df['ward'] = ward\n","            df_list.append(df)\n","        else:\n","            print(f\"Warning: File {file_name} does not exist and will be skipped.\")\n","\n","    if df_list:\n","        concatenated_df = pd.concat(df_list, ignore_index=True)\n","        columns = ['ward'] + [col for col in concatenated_df.columns if col != 'ward']\n","        concatenated_df = concatenated_df[columns]\n","        return concatenated_df\n","    else:\n","        print(\"No files to concatenate.\")\n","        return pd.DataFrame()  # Return an empty DataFrame if no files were found"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fe7KGUtomwhj"},"outputs":[],"source":["# Function to process client data\n","# def process_clients(df_clients, df_scenarios):\n","#     # Create a unique client ID\n","#     df_clients['ct_id'] = df_clients.index + 1\n","#     # Get the complications and the number of months. These are duplicate rows in the scenarios df\n","#     unique_scen = df_scenarios[['ward', 'client_id', 'complications', 'num_months']].drop_duplicates()\n","#     df_clients = df_clients.merge(unique_scen, on=['ward', 'client_id'], how='left')\n","#     return df_clients\n","\n","def process_clients(df_clients, df_scenarios):\n","    df_clients = (df_clients\n","                  .assign(ct_id=df_clients.index + 1)\n","                  .merge(df_scenarios[['ward', 'client_id', 'complications', 'num_months']].drop_duplicates(),\n","                         on=['ward', 'client_id'], how='left'))\n","    return df_clients\n","\n","def rename_client_columns(df_clients):\n","    # Rename and reorder columns for df_clients. \n","    df_clients = (df_clients\n","                  .rename(columns={\n","                      'naam': 'name',\n","                      'type_dementie': 'dementia_type',\n","                      'somatiek': 'physical',\n","                      'adl': 'adl',\n","                      'mobiliteit': 'mobility',\n","                      'gedrag': 'behavior',\n","                  })\n","                  [['ct_id', 'ward', 'name', 'dementia_type', 'physical', 'adl',\n","                    'mobility', 'behavior', 'complications', 'num_months']])\n","    return df_clients\n","\n","# Function to process scenarios data\n","def process_scenarios(df_scenarios, df_clients):\n","    df_scenarios = (df_scenarios\n","                    # Complications and num_months have been added to the clients df\n","                    .drop(columns=['complications', 'num_months'])\n","                    # Get the unique client ID\n","                    .merge(df_clients[['ward', 'client_id', 'ct_id']], on=['ward', 'client_id'], how='left')\n","                    # Extract the month number from the string\n","                    .assign(month=lambda df: df['month'].str.extract('(\\d+)').astype(int))\n","                    .rename(columns={'journey': 'scenario'})\n","                    [['ct_id', 'month', 'scenario']])\n","    return df_scenarios\n","\n","# Function to process records data\n","def process_records(df_records, df_clients):\n","    df_records = (df_records\n","                  .merge(df_clients[['ward', 'client_id', 'ct_id']], on=['ward', 'client_id'], how='left')\n","                  .rename(columns={'dag': 'day', 'tijd': 'time', 'rapportage': 'note'})\n","                  [['ct_id', 'month', 'iteration', 'day', 'time', 'note']])\n","    return df_records\n","\n","# Function to process summaries data\n","def process_summaries(df_summaries, df_clients):\n","    summaries_list = df_summaries['summary'].tolist()\n","    dict_list = [ast.literal_eval(s) for s in summaries_list]\n","    df_parsed_summaries = pd.DataFrame(dict_list).drop(columns=['client_id'])\n","\n","    df_summaries = (df_summaries\n","                    .merge(df_clients[['ward', 'client_id', 'ct_id']], on=['ward', 'client_id'], how='left')\n","                    [['ct_id']])\n","\n","    df_summaries = pd.concat([df_summaries, df_parsed_summaries], axis=1)\n","    return df_summaries\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to convert DataFrames to Hugging Face Datasets and push to hub\n","def push_dataset_to_hub(df, dataset_name, hf_repo_name):\n","    dataset = Dataset.from_pandas(df)\n","    dataset.push_to_hub(f\"{hf_repo_name}_{dataset_name}\", private=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9bLmbwqPrYae"},"outputs":[],"source":["# Main script execution\n","def main():\n","    df_clients = concatenate_files(DATA_DIR, wards, 'gcai_client_profiles')\n","    df_scenarios = concatenate_files(DATA_DIR, wards, 'gcai_client_scenarios')\n","    df_records = concatenate_files(DATA_DIR, wards, 'gcai_client_notes')\n","    df_summaries = concatenate_files(DATA_DIR, wards, 'gcai_client_summaries')\n","\n","    df_clients = process_clients(df_clients, df_scenarios)\n","    df_scenarios = process_scenarios(df_scenarios, df_clients)\n","    df_records = process_records(df_records, df_clients)\n","    df_summaries = process_summaries(df_summaries, df_clients)\n","    df_clients = rename_client_columns(df_clients)\n","\n","    # Save final processed dataframes\n","    df_clients.to_csv(FN_CLIENTS_DF, index=False)\n","    df_scenarios.to_csv(FN_SCENARIOS_DF, index=False)\n","    df_records.to_csv(FN_RECORDS_DF, index=False)\n","    df_summaries.to_csv(FN_SUMMARIES_DF, index=False)\n","    \n","    # Push each dataset to Hugging Face\n","    push_dataset_to_hub(df_summaries, \"summaries\", hf_repo_name)\n","    push_dataset_to_hub(df_records, \"records\", hf_repo_name)\n","    push_dataset_to_hub(df_scenarios, \"scenarios\", hf_repo_name)\n","    push_dataset_to_hub(df_clients, \"clients\", hf_repo_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZ-KoJb5oiHm"},"outputs":[],"source":["if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S2BHfPV6pFvO"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNMXJnj2wcGGpmwsb0ADUwU","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
