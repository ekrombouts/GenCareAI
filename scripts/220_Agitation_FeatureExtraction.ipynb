{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Agitation prediction: Feature extraction\n","\n","**Author:** Eva Rombouts  \n","**Date:** 2024-07-19  \n","**Version:** 1.0\n","\n","### Description\n","This script performs text preprocessing, topic modeling, and word embedding training on a dataset of care reports. It produces three output files: agitation_train, agitation_valid, and agitation_test, each containing extracted features. These features are prepared for subsequent modeling to predict agitation based on the text data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S5ye0sPteEVq","outputId":"340e72dc-2b6d-41cb-c162-0d3b7798fe72"},"outputs":[],"source":["# Environment setup\n","import os\n","\n","def check_environment():\n","    try:\n","        import google.colab\n","        return \"Google Colab\"\n","    except ImportError:\n","        return \"Local Environment\"\n","\n","env = check_environment()\n","if env == \"Google Colab\":\n","    print(\"Running in Google Colab\")\n","    !python -m spacy download nl_core_news_sm -q\n","    !pip install -q pyLDAvis\n","    from google.colab import drive, userdata\n","    drive.mount('/content/drive')\n","    os.chdir('/content/drive/My Drive/Colab Notebooks/GenCareAI/scripts')\n","else:\n","    print(\"Running in Local Environment\")\n","    # Unable to get it working locally due to persistent ImportError with SciPy's 'triu' function.\n","    # Therefore, switching to Colab where the environment is pre-configured and works seamlessly.\n","    # !pip install -q gensim spacy wordcloud pyLDAvis\n","    # !python -m spacy download nl_core_news_sm -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TOW7CZtPeEVt"},"outputs":[],"source":["# Libraries for data manipulation \n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","tqdm.pandas()\n","\n","# Visualisation libraries\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","import pyLDAvis.gensim_models as gensimvis\n","import pyLDAvis\n","\n","# NLP & Machine Learning libraries\n","import spacy\n","import gensim\n","from gensim import corpora\n","from gensim.models import Word2Vec\n","\n","# Parameters \n","num_topics = 10\n","sample_size = 1000\n","seed = 6\n","no_below = 2   # Infrequency limit, minimum number of times the word needs to be in the texts\n","no_above = 0.5 # Frequency limit, if a word appears in more percent of the texts it will be filtered out.\n","\n","# Dutch SpaCy model\n","nlp = spacy.load('nl_core_news_sm', disable=['parser', 'tagger', 'ner'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-e6Str_BLGx8"},"outputs":[],"source":["# Load and sample data\n","df = pd.read_csv('../data/agitation.csv')\n","# df = df.sample(sample_size, random_state=seed)\n","df.info()"]},{"cell_type":"markdown","metadata":{"id":"tmPb-AbSLGx9"},"source":["## Text preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hBrs0cziLGx_"},"outputs":[],"source":["# Function for text preprocessing: lowercasing, tokenization, lemmatization, stop word removal, and word selection based on part-of-speech\n","def preprocess_text(text, nlp_model):\n","    doc = nlp_model(text)\n","    cleaned_tokens = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop and token.pos_ in ['VERB', 'NOUN', 'ADJ', 'ADV', 'INTJ']]\n","    return \" \".join(cleaned_tokens)\n","\n","# Apply preprocessing to the dataframe\n","df['text_clean'] = df['text'].progress_apply(lambda x: preprocess_text(x, nlp))"]},{"cell_type":"markdown","metadata":{"id":"t3YHHHScLGx_"},"source":["## Splitten in train/test/validatie"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OjV-EyiLGx_"},"outputs":[],"source":["# Split into train/test/validation sets\n","train_df, temp = train_test_split(df, test_size=0.4, random_state=seed)\n","valid_df, test_df = train_test_split(temp, test_size=0.5, random_state=seed)\n","\n","train_df.info()"]},{"cell_type":"markdown","metadata":{"id":"O9YOrfUSMvtD"},"source":["## Topic modelling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4lBWsdtdLGyA"},"outputs":[],"source":["# Function to plot word clouds for topics\n","def plot_wordclouds(lda_model, dictionary):\n","    num_topics = lda_model.num_topics\n","    cols = 5\n","    rows = (num_topics // cols) + (num_topics % cols > 0)  # Berekent het aantal rijen\n","\n","    plt.figure(figsize=(20, 10))  # Aanpassen van de breedte en hoogte van de plot\n","    for idx in range(num_topics):\n","        plt.subplot(rows, cols, idx + 1)\n","        topic_words = dict(lda_model.show_topic(idx, 30))\n","        cloud = WordCloud(background_color='white').generate_from_frequencies(topic_words)\n","        plt.imshow(cloud, interpolation='bilinear')\n","        plt.axis('off')\n","        plt.title('Topic ' + str(idx+1))\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BmZ1Acx6LGyA"},"outputs":[],"source":["# Het model wordt gemaakt obv de trainingset, en vervolgens toegepast op alle data\n","\n","# Tokenize cleaned texts\n","tokenized_docs_train = [[token.text for token in nlp(doc)] for doc in train_df['text_clean']]\n","tokenized_docs_test = [[token.text for token in nlp(doc)] for doc in test_df['text_clean']]\n","tokenized_docs_valid = [[token.text for token in nlp(doc)] for doc in valid_df['text_clean']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UY_L89ecLGyB"},"outputs":[],"source":["# Create a dictionary\n","dictionary = corpora.Dictionary(tokenized_docs_train)\n","dictionary.filter_extremes(no_below=no_below, no_above=no_above)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aDkZzWsjLGyB"},"outputs":[],"source":["# Convert documents to a bag-of-words representation\n","corpus_train = [dictionary.doc2bow(doc) for doc in tokenized_docs_train]\n","corpus_test = [dictionary.doc2bow(doc) for doc in tokenized_docs_test]\n","corpus_valid = [dictionary.doc2bow(doc) for doc in tokenized_docs_valid]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DWD5nBzeL10U"},"outputs":[],"source":["# Train LDA model. The model is trained on the training set only\n","lda_model = gensim.models.LdaMulticore(corpus_train, num_topics=num_topics, id2word=dictionary, passes=10, workers=2, random_state=seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E-NYrcbUL3eG"},"outputs":[],"source":["# Plot topics\n","plot_wordclouds(lda_model, dictionary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hc4Po3qXL5oo"},"outputs":[],"source":["# Visualize topics with pyLDAvis\n","lda_display = gensimvis.prepare(lda_model, corpus_train, dictionary, sort_topics=False)\n","pyLDAvis.display(lda_display)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15njTjp_MEAv"},"outputs":[],"source":["# Function to calculate topic distributions\n","def get_topic_distributions(lda_model, corpus):\n","    topic_distributions = []\n","\n","    for doc_bow in corpus:\n","        doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n","        topic_distribution = {f'topic_{i}': 0 for i in range(num_topics)}\n","        for topic, prob in doc_topics:\n","            topic_distribution[f'topic_{topic}'] = prob\n","        topic_distributions.append(topic_distribution)\n","\n","    return topic_distributions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UMfxm8bCMjqH"},"outputs":[],"source":["# Calculate topic distributions for all documents\n","train_topic_dists = get_topic_distributions(lda_model, corpus_train)\n","test_topic_dists = get_topic_distributions(lda_model, corpus_test)\n","valid_topic_dists = get_topic_distributions(lda_model, corpus_valid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5f7qUjPtMmGh"},"outputs":[],"source":["# Add topic distributions to DataFrames\n","for topic in range(num_topics):\n","    train_df[f'topic_{topic}'] = [dist[f'topic_{topic}'] for dist in train_topic_dists]\n","    test_df[f'topic_{topic}'] = [dist[f'topic_{topic}'] for dist in test_topic_dists]\n","    valid_df[f'topic_{topic}'] = [dist[f'topic_{topic}'] for dist in valid_topic_dists]"]},{"cell_type":"markdown","metadata":{"id":"VeyzXrE1MoNg"},"source":["## Word embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBGLnjAaM1Kc"},"outputs":[],"source":["# Prepare text data for word embeddings\n","train_texts = [doc.split() for doc in train_df['text_clean']]  # Splits in tokens\n","# Train Word2Vec-model\n","word2vec_model = Word2Vec(sentences=train_texts, vector_size=50, window=5, min_count=1, workers=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JUHCHOUWM59B"},"outputs":[],"source":["# Function to calculate document embeddings\n","def calculate_document_embedding(text, model):\n","    embeddings = [model.wv[word] for word in text.split() if word in model.wv]\n","\n","    if not embeddings:\n","        return pd.Series(np.zeros(model.vector_size))\n","\n","    mean_embedding = np.mean(embeddings, axis=0)\n","    return pd.Series(mean_embedding)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RgeKMlDjM7dy"},"outputs":[],"source":["# Calculate document embeddings and add to DataFrames\n","embedding_columns = [f'embedding_{i}' for i in range(word2vec_model.vector_size)]\n","train_df[embedding_columns] = train_df['text_clean'].apply(lambda x: calculate_document_embedding(x, word2vec_model))\n","\n","test_df[embedding_columns] = test_df['text_clean'].apply(lambda x: calculate_document_embedding(x, word2vec_model))\n","valid_df[embedding_columns] = valid_df['text_clean'].apply(lambda x: calculate_document_embedding(x, word2vec_model))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wtI3vN4RM9v_"},"outputs":[],"source":["# Example usage of the trained model\n","word_embedding = word2vec_model.wv['moeder']\n","print(word_embedding)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eNWN-yJSM_4F"},"outputs":[],"source":["# Find words most similar to 'toilet'\n","similar_words = word2vec_model.wv.most_similar('toilet', topn=10)\n","\n","for word, similarity in similar_words:\n","    print(f\"Word: {word}, Similarity: {similarity}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qEKCV1-PNI2M"},"outputs":[],"source":["# Save DataFrames\n","train_df.to_csv('../data/agitation_train.csv', index=False)\n","test_df.to_csv('../data/agitation_test.csv', index=False)\n","valid_df.to_csv('../data/agitation_valid.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WxdcgMZvNT5h"},"outputs":[],"source":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1vblS7-WFaa2ea2KstKhGyMOx_fWVXD-t","timestamp":1721372208480}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
