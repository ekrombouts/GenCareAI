{"cells":[{"cell_type":"markdown","metadata":{"id":"j_rvKrJtCkfP"},"source":["# Summarisation: Train T5 model\n","\n","**Author:** Eva Rombouts  \n","**Date:** 2024-07-21  \n","**Version:** 0.2\n","\n","### Description\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S94qZ2TyZu2P"},"outputs":[],"source":["# Install necessary libraries\n","%%capture\n","!pip install datasets\n","!pip install transformers\n","!pip install transformers[torch]\n","!pip install evaluate\n","!pip install peft\n","!pip install rouge_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pleHCz0cZ60Q"},"outputs":[],"source":["# Mount Google Drive and set up environment\n","import os\n","from google.colab import drive, userdata\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Change directory to your project folder\n","os.chdir('/content/drive/My Drive/Colab Notebooks/GenCareAI/scripts')\n","\n","# Retrieve Hugging Face token\n","HF_TOKEN = userdata.get('HF_TOKEN')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORoJ98bKaGOk"},"outputs":[],"source":["# Import necessary libraries\n","from datasets import load_dataset, Dataset, DatasetDict\n","from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n","import torch\n","import pandas as pd\n","import numpy as np\n","from peft import LoraConfig, get_peft_model, TaskType\n","from sklearn.model_selection import train_test_split\n","import evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D8_pZIHfaJNI"},"outputs":[],"source":["# Load and preprocess data\n","\n","# Load the dataset\n","summaries_df = pd.read_csv('../data/galaxy_summaries.csv')\n","\n","# Set parameters\n","random_seed = 6\n","sample_size = 1000\n","\n","# Sample the dataset\n","# df = summaries_df.sample(sample_size, random_state=random_seed)\n","df = summaries_df\n","\n","# Split data into train, validation, and test sets\n","train_df, temp_df = train_test_split(df, test_size=0.3, random_state=random_seed)\n","valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=random_seed)\n","\n","# Convert pandas DataFrames to Hugging Face Dataset objects\n","train_dataset = Dataset.from_pandas(train_df)\n","valid_dataset = Dataset.from_pandas(valid_df)\n","test_dataset = Dataset.from_pandas(test_df)\n","\n","# Create a DatasetDict\n","dataset_dict = DatasetDict({\n","    'train': train_dataset,\n","    'validation': valid_dataset,\n","    'test': test_dataset\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I1LOi4YGaLN5"},"outputs":[],"source":["# Load the model and tokenizer\n","model_name = 'flax-community/t5-base-dutch-demo'\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Move model to GPU\n","model.to('cuda')\n","\n","# Display model configuration\n","model.config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2pafAUbeaMIW"},"outputs":[],"source":["# Analyze token lengths\n","# Tokenize the input reviews\n","# token_lengths = [len(tokenizer.encode(review, add_special_tokens=True)) for review in df['input']]\n","token_lengths = [len(tokenizer.encode(review, add_special_tokens=True)) for review in df['summary']]\n","\n","# Calculate token length statistics\n","max_length = np.max(token_lengths)\n","mean_length = np.mean(token_lengths)\n","median_length = np.median(token_lengths)\n","percentile_95_length = np.percentile(token_lengths, 95)\n","\n","print(f\"Max length: {max_length}\")\n","print(f\"Mean length: {mean_length}\")\n","print(f\"Median length: {median_length}\")\n","print(f\"95th percentile length: {percentile_95_length}\")\n","\n","# Set chosen max length\n","chosen_max_length = int(percentile_95_length)\n","print(f\"Chosen max length: {chosen_max_length}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_FVCpR8qaMNi"},"outputs":[],"source":["# Tokenize and filter the dataset for fine-tuning\n","start_prompt = 'Schrijf een samenvatting:\\n'\n","end_prompt = '\\n\\nSamenvatting:\\n'\n","\n","def tokenize_function(example, prompt_start, prompt_end):\n","    # Tokenize the data for fine-tuning\n","    prompt = [prompt_start + review + prompt_end for review in example[\"input\"]]\n","    example['input_ids'] = tokenizer(prompt, padding=True, truncation=True, max_length=600, return_tensors=\"pt\").input_ids\n","    example['labels'] = tokenizer(example[\"summary\"], padding=True, truncation=True, max_length=200, return_tensors=\"pt\").input_ids\n","    return example\n","\n","# Apply tokenization and filter columns\n","tokenized_datasets = dataset_dict.map(\n","    lambda example: tokenize_function(example, start_prompt, end_prompt),\n","    batched=True\n",").remove_columns(['input'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IU8MOZNaaMdC"},"outputs":[],"source":["# Set up PEFT configuration\n","peft_config = LoraConfig(\n","    r=32,  # Rank\n","    lora_alpha=16,\n","    target_modules=[\"q\", \"v\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=TaskType.SEQ_2_SEQ_LM  # Specify task type\n",")\n","\n","# Create PEFT model\n","peft_model = get_peft_model(model, peft_config)\n","peft_model.to('cuda')\n","\n","# Display trainable parameters\n","peft_model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ycKrWTu8bijS"},"outputs":[],"source":["# Training setup for PEFT model\n","output_dir = '../models/peft_model'\n","training_args = TrainingArguments(\n","    output_dir=output_dir,\n","    auto_find_batch_size=True,\n","    learning_rate=1e-3,\n","    num_train_epochs=1,\n","    logging_steps=100,\n","    save_steps=200,\n","    eval_steps=200,\n",")\n","\n","# Set up the trainer\n","trainer = Trainer(\n","    model=peft_model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n",")\n","\n","# Train the PEFT model\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XhM6xL6pb5Gd"},"outputs":[],"source":["# Evaluate the PEFT model qualitatively\n","index = 10\n","input_text = dataset_dict['test'][index]['input']\n","baseline_summary = dataset_dict['test'][index]['summary']\n","\n","prompt = start_prompt + input_text + end_prompt\n","\n","# Tokenize the input for the PEFT model\n","input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n","\n","# Generate summaries with the original and PEFT models\n","original_model_outputs = model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n","original_model_summary = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n","\n","peft_model_outputs = trainer.model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n","peft_model_summary = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n","\n","# Print results\n","dash_line = '-' * 100\n","print(dash_line)\n","print(f'PROMPT:\\n{prompt}')\n","print(dash_line)\n","print(f'BASELINE SUMMARY:\\n{baseline_summary}')\n","print(dash_line)\n","print(f'ORIGINAL MODEL SUMMARY:\\n{original_model_summary}')\n","print(dash_line)\n","print(f'PEFT MODEL SUMMARY: {peft_model_summary}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vo5esyxccGtX"},"outputs":[],"source":["# Evaluate the PEFT model quantitatively with ROUGE\n","# Select a subset of reviews and their summaries\n","test_inputs = dataset_dict['test'][0:10]['input']\n","baseline_summaries = dataset_dict['test'][0:10]['summary']\n","\n","# Initialize lists for generated summaries\n","original_model_summaries = []\n","peft_model_summaries = []\n","\n","# Generate summaries for the subset of reviews\n","for idx, review in enumerate(test_inputs):\n","    prompt = start_prompt + review + end_prompt\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n","\n","    original_model_output = model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n","    original_model_summary = tokenizer.decode(original_model_output[0], skip_special_tokens=True)\n","    original_model_summaries.append(original_model_summary)\n","\n","    peft_model_output = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n","    peft_model_summary = tokenizer.decode(peft_model_output[0], skip_special_tokens=True)\n","    peft_model_summaries.append(peft_model_summary)\n","\n","# Create a DataFrame with the summaries\n","summary_df = pd.DataFrame(list(zip(baseline_summaries, original_model_summaries, peft_model_summaries)),\n","                          columns=['Baseline Summaries', 'Original Model Summaries', 'PEFT Model Summaries'])\n","\n","# Calculate ROUGE scores for each model\n","rouge = evaluate.load('rouge')\n","original_model_rouge = rouge.compute(predictions=original_model_summaries, references=baseline_summaries, use_aggregator=True, use_stemmer=True)\n","peft_model_rouge = rouge.compute(predictions=peft_model_summaries, references=baseline_summaries, use_aggregator=True, use_stemmer=True)\n","\n","# Print the ROUGE scores\n","print('ORIGINAL MODEL ROUGE SCORES:')\n","print(original_model_rouge)\n","print('PEFT MODEL ROUGE SCORES:')\n","print(peft_model_rouge)\n","\n","# Calculate and print the improvement of the PEFT model over the original model\n","improvement = (np.array(list(peft_model_rouge.values())) - np.array(list(original_model_rouge.values())))\n","print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n","for key, value in zip(peft_model_rouge.keys(), improvement):\n","    print(f'{key}: {value*100:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iLd33PF3cTQS"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
