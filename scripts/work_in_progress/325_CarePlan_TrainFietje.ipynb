{"cells":[{"cell_type":"markdown","metadata":{"id":"qJ-jErPJmWy4"},"source":["# Care_Plan: Train Fietje model\n","\n","**Author:** Eva Rombouts  \n","**Date:** 2024-07-28  \n","**Version:** 0.2\n","\n","### Description\n","This notebook is almost fully copied from: [Optimizing Phi-2: A Deep Dive into Fine-Tuning Small Language Models](https://medium.com/thedeephub/optimizing-phi-2-a-deep-dive-into-fine-tuning-small-language-models-9d545ac90a99), by Praveen Yerneni. Thank you!!\n","It trains the chat version of [Fietje](https://huggingface.co/BramVanroy/fietje-2-chat), an adapated version of microsoft/phi-2, trained on Dutch texts."]},{"cell_type":"markdown","metadata":{"id":"SyLKlyMkORZO"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQxgMZ32pzx-"},"outputs":[],"source":["# #Install the required packages\n","!pip install -q bitsandbytes flash_attn datasets peft\n","# !pip install einops  bitsandbytes accelerate peft flash_attn\n","# !pip uninstall -y transformers\n","# !pip install git+https://github.com/huggingface/transformers\n","# !pip install --upgrade torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QpsFD-hEMMo6"},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer\n","from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n","\n","import os\n","from google.colab import drive\n","from sklearn.model_selection import train_test_split\n","from datasets import Dataset\n","import pandas as pd\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZWtnR2WMPR-"},"outputs":[],"source":["model_name = \"BramVanroy/fietje-2b-chat\"\n","model_finetuned = \"fietje_zorgplan_magweg\" #Name of the model to save to HuggingFace hub\n","commit_message = \"Fietje finetuned\"\n","random_seed = 6\n","sample_size = 100"]},{"cell_type":"markdown","metadata":{"id":"LfTAS_SRfJ6y"},"source":["## Load model and tokenizer\n","Model: [fietje-2-chat](https://huggingface.co/BramVanroy/fietje-2-chat)\n","\n","The model is loaded in `4-bit` which is the \"Quantization\" part of QLORA. The memory footprint of this is much smaller then the default.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jCpMhnh0hh0t"},"outputs":[],"source":["# Configuration to load model in 4-bit quantized\n","bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n","                                bnb_4bit_quant_type='nf4',\n","                                bnb_4bit_compute_dtype='float16',\n","                                #bnb_4bit_compute_dtype=torch.bfloat16,\n","                                bnb_4bit_use_double_quant=True)\n","\n","\n","#Loading the model with compatible settings\n","model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto',\n","                                             quantization_config=bnb_config,\n","                                             attn_implementation='flash_attention_2',\n","                                             trust_remote_code=True)\n","\n","# Setting up the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name,\n","                                          add_eos_token=True,\n","                                          trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.truncation_side = 'left'\n","\n","print(f\"Memory footprint: {model.get_memory_footprint() / 1e9} GB\")"]},{"cell_type":"markdown","metadata":{"id":"w9d7qab_MQu7"},"source":["## Prepare data\n","\n","ToDo: Publish dataset on HF. [link text](*https://*)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"teSp0jIXzedK"},"outputs":[],"source":["# Mount Google Drive\n","drive.mount('/content/drive')\n","# Change directory to your project folder\n","os.chdir('/content/drive/My Drive/Colab Notebooks/GenCareAI/scripts')\n","\n","# # Retrieve Hugging Face token\n","# HF_TOKEN = userdata.get('HF_TOKEN')\n","\n","# Load and preprocess data\n","# Load the dataset\n","df_careplans = pd.read_csv('../data/df_careplans.csv')\n","# Sample the dataset, rename columns and keep only input & output column\n","df = (df_careplans\n","      .sample(sample_size,random_state=random_seed)\n","      .rename(columns={'notes': 'input', 'careplan': 'output'})\n","      [['input','output']]\n",")\n","\n","# Split data into train, and test sets\n","train_df, test_df = train_test_split(df, test_size=0.15, random_state=random_seed)\n","\n","# Convert to Hugging Face Dataset objects\n","train_dataset = Dataset.from_pandas(train_df)\n","test_dataset = Dataset.from_pandas(test_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5MG4lf6znqX"},"outputs":[],"source":["print(train_dataset)\n","print(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cjiMMZ-gkotJ"},"outputs":[],"source":["#Function that creates a prompt from input and output and tokenizes it\n","def collate_and_tokenize(examples):\n","\n","    careplan = examples[\"output\"][0].replace('\"', r'\\\"')\n","    notes = examples[\"input\"][0]\n","\n","    #Merging into one prompt for tokenization and training\n","    prompt = f'''###System:\n","Lees de gegeven rapportages en schrijf een zorgplan volgens de instructies.\n","###Rapportages:\n","{notes}\n","###Instructies:\n","Schrijf een zorgplan voor de drie belangrijkste zorgproblemen op basis van de rapportages.\n","Formaat: Probleem, Doel, Acties\n","###Zorgplan:\n","{careplan}\n","'''\n","\n","    #Tokenize the prompt\n","    encoded = tokenizer(\n","        prompt,\n","        return_tensors=\"np\",\n","        padding=\"max_length\",\n","        truncation=True,\n","        ## Very critical to keep max_length at 1024.\n","        ## Anything more will lead to OOM on T4\n","        max_length=2048,\n","    )\n","\n","    encoded[\"labels\"] = encoded[\"input_ids\"]\n","    return encoded"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RZLVRL2Wk3vj"},"outputs":[],"source":["#We will just keep the input_ids and labels that we add in function above.\n","columns_to_remove = [\"input\",\"output\", \"__index_level_0__\"]\n","\n","#tokenize the training and test datasets\n","tokenized_dataset_train = train_dataset.map(collate_and_tokenize,\n","                                            batched=True,\n","                                            batch_size=1,\n","                                            remove_columns=columns_to_remove)\n","tokenized_dataset_test = test_dataset.map(collate_and_tokenize,\n","                                          batched=True,\n","                                          batch_size=1,\n","                                          remove_columns=columns_to_remove)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tez7pYN4jqTF"},"outputs":[],"source":["#Check if tokenization looks good\n","input_ids = tokenized_dataset_test[1]['input_ids']\n","\n","decoded = tokenizer.decode(input_ids, skip_special_tokens=True)\n","print(decoded)"]},{"cell_type":"markdown","metadata":{"id":"rI5RazFBvt6e"},"source":["## Train model"]},{"cell_type":"markdown","metadata":{"id":"FGfj4f7qPZil"},"source":["### Prepare model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vB9NMBzasFcT"},"outputs":[],"source":["#Accelerate training models on larger batch sizes, we can use a fully sharded data parallel model.\n","from accelerate import FullyShardedDataParallelPlugin, Accelerator\n","from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n","\n","fsdp_plugin = FullyShardedDataParallelPlugin(\n","    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n","    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",")\n","\n","accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dipMfTbgPvoM"},"outputs":[],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uamQX0JVvoGF"},"outputs":[],"source":["print_trainable_parameters(model)\n","\n","#gradient checkpointing to save memory\n","model.gradient_checkpointing_enable()\n","\n","# Freeze base model layers and cast layernorm in fp32\n","model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3PuVk7YBwxY9"},"outputs":[],"source":["config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    target_modules=[\n","    'q_proj',\n","    'k_proj',\n","    'v_proj',\n","    'dense',\n","    'fc1',\n","    'fc2',\n","    ], #print(model) will show the modules to use\n","    bias=\"none\",\n","    lora_dropout=0.05,\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","lora_model = get_peft_model(model, config)\n","print_trainable_parameters(lora_model)\n","\n","lora_model = accelerator.prepare_model(lora_model)"]},{"cell_type":"markdown","metadata":{"id":"Ro0WaZpJhhvp"},"source":["### Train and save model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m2T1Q6Zz9k3H"},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir='./results',  # Output directory for checkpoints and predictions\n","    report_to='none',\n","    overwrite_output_dir=True, # Overwrite the content of the output directory\n","    per_device_train_batch_size=2,  # Batch size for training\n","    per_device_eval_batch_size=2,  # Batch size for evaluation\n","    gradient_accumulation_steps=5, # number of steps before optimizing\n","    gradient_checkpointing=True,   # Enable gradient checkpointing\n","    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n","    warmup_steps=50,  # Number of warmup steps\n","    #max_steps=1000,  # Total number of training steps\n","    num_train_epochs=2,  # Number of training epochs\n","    learning_rate=5e-5,  # Learning rate\n","    weight_decay=0.01,  # Weight decay\n","    optim=\"paged_adamw_8bit\", #Keep the optimizer state and quantize it\n","    fp16=True, #Use mixed precision training\n","    #For logging and saving\n","    logging_dir='./logs',\n","    logging_strategy=\"steps\",\n","    logging_steps=100,\n","    save_strategy=\"steps\",\n","    save_steps=100,\n","    save_total_limit=2,  # Limit the total number of checkpoints\n","    eval_strategy=\"steps\",\n","    eval_steps=100,\n","    load_best_model_at_end=True, # Load the best model at the end of training\n",")\n","\n","trainer = Trainer(\n","    model=lora_model,\n","    train_dataset=tokenized_dataset_train,\n","    eval_dataset=tokenized_dataset_test,\n","    args=training_args,\n",")\n","\n","#Disable cache to prevent warning, reenable for inference\n","model.config.use_cache = False\n","\n","start_time = time.time()  # Record the start time\n","trainer.train()  # Start training\n","end_time = time.time()  # Record the end time\n","\n","training_time = end_time - start_time  # Calculate total training time\n","\n","print(f\"Training completed in {training_time} seconds.\")\n","\n","#Save model to hub to ensure we save our work.\n","lora_model.push_to_hub(model_finetuned,\n","                  use_auth_token=True,\n","                  commit_message=commit_message,\n","                  private=True)\n","\n","\n","#Terminate the session so we do not incur cost\n","from google.colab import runtime\n","runtime.unassign()"]},{"cell_type":"markdown","metadata":{"id":"8hFerF5h-nQS"},"source":["# Run Inference\n","\n","**Note**: Ensure to stop your session and reconnect and reload the model before running the code below.\n","\n","First we will run inference without the trained weights and check the output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OvcEl1z2wRdY"},"outputs":[],"source":["#Setup a prompt that we can use for testing\n","\n","new_prompt = \"\"\"##System:\n","Lees de gegeven rapportages en schrijf een zorgplan volgens de instructies.\n","###Rapportages:\n","- Mevrouw toonde vanochtend meer verwardheid en desoriëntatie. Begeleiding aangepast en extra ondersteuning geboden bij het uitvoeren van dagelijkse activiteiten.\n","- Mevrouw had behoefte aan extra rust na de ochtendactiviteiten. Tijd genomen voor ontspanning en rustgevende omgeving gecreëerd.\n","- Mevrouw had moeite met het herkennen van bekende gezichten. Begeleiding gegeven bij het omgaan met desoriëntatie en geheugenverlies.\n","- Mevrouw Vos was vanochtend rustiger en minder verward. Positieve interacties gehad tijdens het ontbijt en ondersteuning geboden bij het aankleden.\n","- Mevrouw had last van benauwdheid door haar astma. Astma-inhalator toegediend en ademhaling gecontroleerd. Bewustzijn en saturatie genormaliseerd.\n","- Mevrouw Vos had extra begeleiding nodig bij het vinden van haar kamer. Vertrouwde zorghandelingen herhaald en geheugenondersteuning geboden.\n","- Mevrouw was vanochtend erg onrustig en verward. Extra aandacht besteed aan rustgevende activiteiten en familiarisatie om haar te kalmeren.\n","- Mevrouw had moeite met het herinneren van recente gebeurtenissen. Geheugentraining en cognitieve stimulatie toegepast om haar geheugen te ondersteunen.\n","- Mevrouw Vos vertoonde tekenen van vermoeidheid en desoriëntatie voor het slapengaan. Begeleiding geboden bij het naar bed gaan en rustige slaapomgeving gecreëerd.\n","###Instructies:\n","Schrijf een zorgplan voor de drie belangrijkste zorgproblemen op basis van de rapportages.\n","Formaat: Probleem, Doel, Acties\n","###Zorgplan:\n","\"\"\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VEIU36d2lyCG"},"outputs":[],"source":["inputs = tokenizer(new_prompt, return_tensors=\"pt\",\n","                   return_attention_mask=False,\n","                   padding=True, truncation=True)\n","\n","inputs.to('cuda')\n","\n","outputs = model.generate(**inputs, repetition_penalty=1.0,\n","                              max_length=1000)\n","result = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","print(result[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wA5_aJxbdedi"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXMhFsxlWyDc"},"outputs":[],"source":["from peft import PeftModel, PeftConfig\n","\n","#Load the model weights from hub\n","model_id = \"ekrombouts/fietje_zorgplan\"\n","trained_model = PeftModel.from_pretrained(model, model_id)\n","\n","#Run inference\n","outputs = trained_model.generate(**inputs, max_length=1000)\n","text = tokenizer.batch_decode(outputs,skip_special_tokens=True)[0]\n","print(text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bXEggIa3Ecvx"},"outputs":[],"source":["index = 3\n","prompt = f'''###System:\n","Lees de gegeven rapportages en schrijf een zorgplan volgens de instructies.\n","###Rapportages:\n","{test_dataset[\"input\"][index]}\n","###Instructies:\n","Schrijf een zorgplan voor de drie belangrijkste zorgproblemen op basis van de rapportages.\n","Formaat: Probleem, Doel, Acties\n","'''\n","\n","inputs = tokenizer(prompt, return_tensors=\"pt\",\n","                   return_attention_mask=False,\n","                   padding=True, truncation=True)\n","\n","inputs.to('cuda')\n","outputs = trained_model.generate(**inputs, max_length=2048)\n","text = tokenizer.batch_decode(outputs,skip_special_tokens=True)[0]\n","print(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6UaRC0YGiCt"},"outputs":[],"source":["prompt = f'''\n","###Rapportages:\n","Maandag\n","S/ Ik voel me zo ziek\n","O/ Mw ziet er grauw uit. Controles gedaan, heeft koorts\n","P/ Arts waarschuwen\n","Dinsdag\n","Arts is geweest, heeft haar antibiotica gegeven bij een longontsteking\n","Woensdag\n","Mw is erg onrustig. Het ziet er niet goed uit. Familie gebeld, die zullen komen.\n","Donderdag\n","Reutelende ademhaling, lage saturatie. Arts is geweest, mw is in de laatste fase. Krijgt wisselligging ter preventie decubitus en morfine en dormicum voor de benauwdheid en onrust.\n","###Instructies:\n","Schrijf een zorgplan voor de drie belangrijkste zorgproblemen op basis van de rapportages.\n","Formaat: Probleem, Doel, Acties\n","'''\n","\n","inputs = tokenizer(prompt, return_tensors=\"pt\",\n","                   return_attention_mask=False,\n","                   padding=True, truncation=True)\n","\n","inputs.to('cuda')\n","outputs = trained_model.generate(**inputs, max_length=1000)\n","text = tokenizer.batch_decode(outputs,skip_special_tokens=True)[0]\n","print(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MTentZfIJcY3"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","name":"326_CarePlan_TrainFietje.ipynb","provenance":[{"file_id":"https://gist.github.com/ekrombouts/58c5fd168bf6a1bde79f0f20b20ff468#file-phi2-finetune-ipynb","timestamp":1722278163343}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
