{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekrombouts/GenCareAI/blob/main/notebooks/100_note_generation/130_RAGIndexing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_63aAoRXj07"
      },
      "source": [
        "# GenCare AI: Retrieval Augmented Generation of care notes\n",
        "\n",
        "**Author:** Eva Rombouts  \n",
        "**Date:** 2024-06-03  \n",
        "**Updated:** 2024-09-30  \n",
        "**Version:** 2.0\n",
        "\n",
        "### Description\n",
        "In this script the anonymous client notes generated [here](https://github.com/ekrombouts/GenCareAI/blob/main/scripts/100_GenerateAnonymousCareNotes.ipynb) are processed and stored in a vector database (Chroma). It enables querying of the database using OpenAI's embeddings and a retrieval-augmented generation system.\n",
        "\n",
        "*Document Loading and Processing*: Documents are loaded from the Hugging Face platform, split into smaller sections by a LangChain Text Splitter, and pre-processed. The notes are split into smaller chunks, even though this probably was not necessary for this dataset. This step was taken for completeness to ensure scalability in the future.  \n",
        "*Database Initialization and Population*: A Chroma vector database is initialized and populated it with the embedded document chunks.  \n",
        "*Query Operations*: The RetrievalQA pipeline let's us search the database using natural language queries, demonstrating the capability to retrieve and display relevant information.\n",
        "\n",
        "### Goal\n",
        "My goal is to use this vector database to retrieve relevant examples for few-shot inference in prompts for creating synthetic client notes. This approach can help me improve the generation of this data by providing specific, contextually relevant examples that guide the model's results.\n",
        "\n",
        "### Setup and configuration\n",
        "- When running in CoLab Google Drive is mounted to persistently store the Chroma vector database.\n",
        "- Retrieve API keys for OpenAI and HuggingFace, providing authentication for accessing the [embedding model](https://platform.openai.com/docs/guides/embeddings), the [QA model](https://platform.openai.com/docs/models) and the [dataset](https://huggingface.co/datasets/ekrombouts/dutch_nursing_home_reports). \n",
        "\n",
        "### Recommended Resources\n",
        "- [RAG - Retrieval Augmented Generation](https://www.youtube.com/playlist?list=PL8motc6AQftn-X1HkaGG9KjmKtWImCKJS) with Sam Witteveen on YouTube\n",
        "- [Python RAG Tutorial (with Local LLMs)](https://www.youtube.com/watch?v=2TJxpyO3ei4&t=323s) by Pixegami on YouTube\n",
        "- And of course the [Langchain documentation](https://python.langchain.com/v0.1/docs/use_cases/question_answering/)\n",
        "\n",
        "***Please note*** that the embedding isn't free. Embedding the 35.000+ notes costs appr $0.15. The costs for the examples of querying the database in this notebook are negligible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install GenCareAI\n",
        "from GenCareAI.GenCareAIUtils import GenCareAISetup\n",
        "\n",
        "setup = GenCareAISetup()\n",
        "\n",
        "if setup.environment == 'Colab':\n",
        "        !pip install -q langchain langchain-openai langchain-community chromadb datasets langchain-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nxPS7wYjrugf"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules from Langchain and Hugging Face\n",
        "import os\n",
        "import pandas as pd\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
        "from langchain_community.document_loaders import HuggingFaceDatasetLoader, DataFrameLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.chains import RetrievalQA\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YCGSQTBMrzxT"
      },
      "outputs": [],
      "source": [
        "# Constants for dataset and storage paths\n",
        "# Use local csv or HuggingFace dataset\n",
        "# path_dataset = setup.get_file_path('data/gcai_notes.csv') \n",
        "path_dataset = \"ekrombouts/Olympia_notes\"\n",
        "\n",
        "path_db_gcai = setup.get_file_path('data/chroma_db_gcai_notes')\n",
        "collection_name = 'anonymous_notes'\n",
        "model = 'text-embedding-ada-002'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l29SPKZNsfuN"
      },
      "outputs": [],
      "source": [
        "def load_documents(path):\n",
        "    \"\"\"Load the dataset either from Hugging Face or a local CSV file based on the path provided.\"\"\"\n",
        "    \n",
        "    try:\n",
        "        # Try to load the Hugging Face dataset       \n",
        "        loader = HuggingFaceDatasetLoader(path, page_content_column='note', use_auth_token=setup.get_hf_token())\n",
        "        return loader.load()\n",
        "    \n",
        "    except Exception:\n",
        "        # If loading as a Hugging Face dataset fails, assume it's a CSV file        \n",
        "        df = pd.read_csv(path)\n",
        "        loader = DataFrameLoader(df, page_content_column='note')\n",
        "        return loader.load()\n",
        "\n",
        "documents = load_documents(path=path_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCRJwVFXuyE9"
      },
      "outputs": [],
      "source": [
        "def split_documents(documents: list[Document]):\n",
        "  \"\"\"Split large text documents into manageable chunks for better handling by ML models.\"\"\"\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size=800,\n",
        "      chunk_overlap=100)\n",
        "  chunks = text_splitter.split_documents(documents)\n",
        "  # Index each chunk to maintain unique identifiers\n",
        "  for idx, chunk in enumerate(chunks):\n",
        "      chunk.metadata['id'] = str(idx)\n",
        "  return chunks\n",
        "\n",
        "chunks = split_documents(documents=documents)\n",
        "\n",
        "print(len(documents))\n",
        "print(len(chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NZKYyBXp7iTp"
      },
      "outputs": [],
      "source": [
        "def initialize_vectordb(persist_directory, embedding_function, collection_name):\n",
        "    \"\"\"Initialize the Chroma vector database, either loading an existing one or creating a new one.\"\"\"\n",
        "    if os.path.exists(persist_directory):\n",
        "        return Chroma(persist_directory=persist_directory,\n",
        "                      embedding_function=embedding_function,\n",
        "                      collection_name=collection_name)\n",
        "    else:\n",
        "        return Chroma(embedding_function=embedding_function,\n",
        "                      persist_directory=persist_directory,\n",
        "                      collection_name=collection_name)\n",
        "\n",
        "# Initialize vector database, using OpenAI embeddings\n",
        "embedding = OpenAIEmbeddings(api_key=setup.get_openai_key(), model=model)\n",
        "vectordb = initialize_vectordb(path_db_gcai, embedding, collection_name)\n",
        "\n",
        "# If you get an error, run this cell again.(TODO fix it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modified from https://github.com/pixegami/rag-tutorial-v2/blob/main/populate_database.py\n",
        "def add_new_documents(vectordb, documents, batch_size=5000):\n",
        "    \"\"\"Add new documents to the database\"\"\"\n",
        "\n",
        "    def load_existing_ids(vectordb):\n",
        "        \"\"\"Fetch existing document IDs from the database to avoid duplicates.\"\"\"\n",
        "        try:\n",
        "            existing_items = vectordb.get(include=[])\n",
        "            existing_ids = set(existing_items[\"ids\"])\n",
        "        except:\n",
        "            existing_ids = set()\n",
        "        return existing_ids\n",
        "\n",
        "    existing_ids = load_existing_ids(vectordb)\n",
        "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
        "\n",
        "    # Only add documents that don't exist in the DB.\n",
        "    new_documents = []\n",
        "    for document in documents:\n",
        "        if document.metadata[\"id\"] not in existing_ids:\n",
        "            new_documents.append(document)\n",
        "\n",
        "    if len(new_documents):\n",
        "        print(f\"Total new documents to add: {len(new_documents)}\")\n",
        "\n",
        "        # Process documents in batches\n",
        "        for i in range(0, len(new_documents), batch_size):\n",
        "            batch = new_documents[i:i + batch_size]\n",
        "            batch_ids = [document.metadata[\"id\"] for document in batch]\n",
        "            vectordb.add_documents(batch, ids=batch_ids)\n",
        "            print(f\"Added batch {i//batch_size + 1} with {len(batch)} documents\")\n",
        "    else:\n",
        "        print(\"No new documents to add\")\n",
        "\n",
        "add_new_documents(vectordb, chunks, batch_size=250)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56otpzQ509Lu"
      },
      "source": [
        "### Query the database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuGkHJ26AhVs"
      },
      "outputs": [],
      "source": [
        "## To read the db from file\n",
        "\n",
        "# vectordb = Chroma(persist_directory=FN_DB_GCAI,\n",
        "#                   embedding_function=OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=model),\n",
        "#                   collection_name = collection_name\n",
        "#                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVUpJYDt2eNQ"
      },
      "outputs": [],
      "source": [
        "# Delete all items in the db\n",
        "\n",
        "# items = vectordb.get(include=[])\n",
        "# existing_ids = items[\"ids\"]\n",
        "# vectordb.delete(ids=existing_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOkFn9g_-XSZ"
      },
      "outputs": [],
      "source": [
        "# Retrieve metadata and document IDs from the database\n",
        "items = vectordb.get(include=['metadatas'])\n",
        "existing_ids = set(items[\"ids\"])\n",
        "metadata = items['metadatas']\n",
        "print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
        "print(metadata[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tEQfxXps_D0x"
      },
      "outputs": [],
      "source": [
        "# Set up a retriever for document querying\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOLn6rsX-k7m"
      },
      "outputs": [],
      "source": [
        "# Query the vector database using similarity search\n",
        "query = 'gewichtsverlies'\n",
        "docs = retriever.invoke(query)\n",
        "\n",
        "print(f'Number of docs: {len(docs)}\\n')\n",
        "print(f'Retriever search type: {retriever.search_type}\\n')\n",
        "\n",
        "print(f'Documents most similar to \"{query}\":')\n",
        "for doc in docs:\n",
        "  print(doc.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hfgNejX3_WrV"
      },
      "outputs": [],
      "source": [
        "# Initialize the QA chain for answering questions using the retrieved documents\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(api_key=setup.get_openai_key()),\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vUHXH3SYCYbI"
      },
      "outputs": [],
      "source": [
        "# function to test the pipeline\n",
        "def query_retrieval_pipeline(query):\n",
        "\n",
        "  def process_llm_response(llm_response):\n",
        "      print(100 * '*')\n",
        "      print(f\"\\nresult: {llm_response['result']}\")\n",
        "      print('\\nSources:')\n",
        "      for source in llm_response[\"source_documents\"]:\n",
        "          print(source.metadata['id'], source.metadata['category'])\n",
        "\n",
        "  llm_response = qa_chain.invoke(query)\n",
        "  pprint(llm_response)\n",
        "  print(process_llm_response(llm_response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GD6uq1UzS8ng"
      },
      "outputs": [],
      "source": [
        "query_retrieval_pipeline (\"Wat moet je doen als je client afvalt in gewicht?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFZ8vW4-C-LW"
      },
      "outputs": [],
      "source": [
        "query_retrieval_pipeline(\"Wat moet je doen als je client agressief gedrag vertoont?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SJAb2aXDg0P"
      },
      "outputs": [],
      "source": [
        "query_retrieval_pipeline(\"Wat kan je doen als een cliënt onrustig is 's nachts?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_V8JtBEEZeA"
      },
      "outputs": [],
      "source": [
        "query_retrieval_pipeline(\"Welke interventies zijn ingezet voor het verbeteren van de nachtrust?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54V-v_euVl5r"
      },
      "outputs": [],
      "source": [
        "query_retrieval_pipeline(\"Wat zijn leuke dingen om te doen met bezoek?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwShUZWgn6Sx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNgBOefXHBMTWJpNUOwkdE5",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
