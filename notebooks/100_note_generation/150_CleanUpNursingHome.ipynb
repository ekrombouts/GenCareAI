{"cells":[{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://colab.research.google.com/github/ekrombouts/GenCareAI/blob/main/notebooks/100_note_generation/150_CleanUpNursingHome.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"ipdYvH-RuXl7"},"source":["# GenCare AI: Concat and clean data\n","\n","**Author:** Eva Rombouts  \n","**Date:** 2024-09-02  \n","**Version:** 1.1\n","\n","### Description\n","This script concatenates the profiles, scenario's, client records and summaries generated in different experiments (each stored as different 'wards') and restructures and cleans up the data.\n","\n","The result is a set of 4 datasets:\n","(If the CARE_HOME variable is set to 'Galaxy':)\n","1. Galaxy_clients\n","2. Galaxy_scenarios\n","3. Galaxy_records\n","4. Galaxy_summaries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhtBxmkAncAM"},"outputs":[],"source":["!pip install GenCareAI\n","from GenCareAI.GenCareAIUtils import GenCareAISetup\n","\n","setup = GenCareAISetup()\n","\n","if setup.environment == 'Colab':\n","        !pip install -q datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nx_U5CkEmsRR"},"outputs":[],"source":["import os\n","import pandas as pd\n","import ast\n","from datetime import datetime, timedelta\n","import random\n","from datasets import Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WFr58E4qm92u"},"outputs":[],"source":["CARE_HOME = 'Galaxy'\n","wards = ['Horizon', 'Venus', 'Tulip', 'Cosmos']\n","\n","FN_CLIENTS_DF = setup.get_file_path(f'data/gcai_{CARE_HOME}_clients.csv')\n","FN_SCENARIOS_DF = setup.get_file_path(f'data/gcai_{CARE_HOME}_scenarios.csv')\n","FN_RECORDS_DF = setup.get_file_path(f'data/gcai_{CARE_HOME}_records.csv')\n","FN_SUMMARIES_DF = setup.get_file_path(f'data/gcai_{CARE_HOME}_summaries.csv')\n","\n","hf_repo_name = \"ekrombouts/\" + CARE_HOME"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def update_dag_counter(df):\n","    \"\"\"\n","    This function updates the 'dag' column in df such that it maintains a running counter per client and per week.\n","    The counter starts at 1 and increments by 1 each time the 'dag' value changes. The counter resets to 1 for each new client and month.\n","    \"\"\"\n","    # Add a column to shift 'dag' values by one row within each group of client_id and month\n","    df['dag_shift'] = df.groupby(['client_id', 'weekno'])['dag'].shift(1)\n","    # Create a column indicating if 'dag' has changed compared to the previous row\n","    df['dag_changed'] = (df['dag'] != df['dag_shift']).astype(int)\n","    # Create a column indicating the start of a new group (client_id and month)\n","    df['group_changed'] = df.groupby(['client_id', 'weekno']).cumcount() == 0\n","    \n","    # Update 'group_changed' to be False if 'dag_shift' is NaN\n","    # df['group_changed'] = df['group_changed'] & df['dag_shift'].notna()\n","    # Create the counter ('teller') by cumulatively summing 'dag_changed' within each group and adding 'group_changed'\n","    df['dag_running'] = df.groupby(['client_id', 'weekno'])['dag_changed'].cumsum() + df['group_changed']\n","    \n","    # Remove the temporary columns used for calculations\n","    # df.drop(columns=['dag_shift', 'dag_changed', 'group_changed'], inplace=True)\n","    \n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def concatenate_files(wards, file_prefix):\n","    df_list = []\n","    \n","    # Loop through each ward to find and read the corresponding file\n","    for ward in wards:\n","        file_name = setup.get_file_path(f'data/{file_prefix}_{ward}.csv')\n","        \n","        # Check if the file exists\n","        if os.path.exists(file_name):\n","            df = pd.read_csv(file_name)  \n","            df['ward'] = ward  # Add a column to indicate the ward\n","            df_list.append(df)  \n","        else:\n","            print(f\"Warning: File {file_name} does not exist and will be skipped.\")  \n","\n","    if df_list:\n","        concatenated_df = pd.concat(df_list, ignore_index=True)\n","        \n","        # Reorder columns to place 'ward' as the first column\n","        columns = ['ward'] + [col for col in concatenated_df.columns if col != 'ward']\n","        concatenated_df = concatenated_df[columns]\n","        \n","        return concatenated_df  \n","    else:\n","        print(\"No files to concatenate.\")  \n","        return pd.DataFrame()  # Return an empty df if no files were found\n","    \n","df_clients = concatenate_files(wards, 'gcai_client_profiles')\n","df_scenarios = concatenate_files(wards, 'gcai_client_scenarios')\n","df_records = concatenate_files(wards, 'gcai_client_notes')\n","df_summaries = concatenate_files(wards, 'gcai_client_summaries')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to process client data: Assigns unique ID to clients and merges with scenario data\n","def process_clients(df_clients, df_scenarios):\n","    df_clients = (df_clients\n","                  .assign(ct_id=df_clients.index + 1)\n","                  .merge(df_scenarios[['ward', 'client_id', 'complications', 'num_months']].drop_duplicates(),\n","                         on=['ward', 'client_id'], how='left'))\n","    return df_clients\n","\n","df_clients = process_clients(df_clients, df_scenarios)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to process scenarios data\n","def process_scenarios(df_scenarios, df_clients):\n","    df_scenarios = (df_scenarios\n","                    # Complications and num_months have been added to the clients df\n","                    .drop(columns=['complications', 'num_months'])\n","                    # Get the unique client ID\n","                    .merge(df_clients[['ward', 'client_id', 'ct_id']], on=['ward', 'client_id'], how='left')\n","                    # Extract the month number from the string\n","                    .assign(month=lambda df: df['month'].str.extract(r'(\\d+)').astype(int))\n","                    .rename(columns={'journey': 'scenario'})\n","                    [['ct_id', 'month', 'scenario']])\n","    return df_scenarios\n","\n","df_scenarios = process_scenarios(df_scenarios, df_clients)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def process_records(df_records, df_clients):\n","    # Generate a random start date for each ct_id\n","    unique_ct_ids = df_clients['ct_id'].unique()\n","    start_dates = {ct_id: datetime(2022, 1, 1) + timedelta(days=random.randint(0, 365)) for ct_id in unique_ct_ids}\n","\n","    # Merge df_records with df_clients to add 'ct_id' to df_records\n","    df_records = df_records.merge(df_clients[['ward', 'client_id', 'ct_id']], on=['ward', 'client_id'], how='left')\n","\n","    # Calculate the maximum 'dag' (day) value for each 'ct_id'\n","    max_days_per_ct_id = df_records.groupby('ct_id')['dag'].max().to_dict()\n","\n","    # Function to calculate the date using the start_date for each ct_id and max 'dag' value\n","    def calculate_datetime(row, max_days_per_ct_id):\n","        max_day = max_days_per_ct_id.get(row['ct_id'], 15)  # Use the max 'dag' value or default to 15\n","        start_date = start_dates[row['ct_id']]  # Get the start date for the specific ct_id\n","        # Combine date and time\n","        base_date = start_date + timedelta(days=(row['month'] - 1) * max_day + (row['dag'] - 1))\n","        # Keep only digits in the time string\n","        time_digits = ''.join(filter(str.isdigit, row['tijd']))\n","        # Convert the cleaned time string to a time object using the '%H%M' format\n","        time_delta = pd.to_datetime(time_digits, format='%H%M').time()\n","        return datetime.combine(base_date, time_delta)\n","\n","    # Apply the datetime calculation to each record and update the DataFrame\n","    df_records = (df_records\n","                  .assign(datetime=lambda df: df.apply(\n","                      lambda row: calculate_datetime(row, max_days_per_ct_id), axis=1))\n","                  .rename(columns={'rapportage': 'note'})\n","                  [['ct_id', 'datetime', 'note']])\n","    \n","    return df_records\n","\n","df_records = process_records(df_records, df_clients)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to process summaries data\n","def process_summaries(df_summaries, df_clients):\n","    summaries_list = df_summaries['summary'].tolist()\n","    dict_list = [ast.literal_eval(s) for s in summaries_list]\n","    df_parsed_summaries = pd.DataFrame(dict_list).drop(columns=['client_id'])\n","\n","    df_summaries = (df_summaries\n","                    .merge(df_clients[['ward', 'client_id', 'ct_id']], on=['ward', 'client_id'], how='left')\n","                    [['ct_id']])\n","\n","    df_summaries = pd.concat([df_summaries, df_parsed_summaries], axis=1)\n","    return df_summaries\n","\n","df_summaries = process_summaries(df_summaries, df_clients)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fe7KGUtomwhj"},"outputs":[],"source":["def rename_client_columns(df_clients):\n","    # Rename and reorder columns for df_clients. \n","    df_clients = (df_clients\n","                  .rename(columns={\n","                      'naam': 'name',\n","                      'type_dementie': 'dementia_type',\n","                      'somatiek': 'physical',\n","                      'adl': 'adl',\n","                      'mobiliteit': 'mobility',\n","                      'gedrag': 'behavior',\n","                  })\n","                  [['ct_id', 'ward', 'name', 'dementia_type', 'physical', 'adl',\n","                    'mobility', 'behavior', 'complications', 'num_months']])\n","    return df_clients\n","\n","df_clients = rename_client_columns(df_clients)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save final processed dataframes\n","df_clients.to_csv(FN_CLIENTS_DF, index=False)\n","df_scenarios.to_csv(FN_SCENARIOS_DF, index=False)\n","df_records.to_csv(FN_RECORDS_DF, index=False)\n","df_summaries.to_csv(FN_SUMMARIES_DF, index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Function to convert DataFrames to Hugging Face Datasets and push to hub\n","# def push_dataset_to_hub(df, dataset_name, hf_repo_name):\n","#     dataset = Dataset.from_pandas(df)\n","#     dataset.push_to_hub(f\"{hf_repo_name}_{dataset_name}\", private=True)\n","\n","# push_dataset_to_hub(df_summaries, \"summaries\", hf_repo_name)\n","# push_dataset_to_hub(df_records, \"records\", hf_repo_name)\n","# push_dataset_to_hub(df_scenarios, \"scenarios\", hf_repo_name)\n","# push_dataset_to_hub(df_clients, \"clients\", hf_repo_name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNMXJnj2wcGGpmwsb0ADUwU","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"gcai","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
