{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNgBOefXHBMTWJpNUOwkdE5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekrombouts/GenCareAI/blob/main/script/RAG/200_RAGIndexing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GenCare AI: Retrieval Augmented Generation of care notes\n",
        "\n",
        "**Author:** Eva Rombouts  \n",
        "**Date:** 2024-06-03  \n",
        "**Version:** 1.0\n",
        "\n",
        "### Description\n",
        "In this script the anonymous client notes generated [here](https://github.com/ekrombouts/GenCareAI/blob/main/scripts/data_generation/100_GenerateCareReportsColab.ipynb) are processed and stored in a vector database (Chroma). It enables efficient querying of the database using OpenAI's embeddings and a retrieval-augmented generation system, which enhances the searchability and accessibility of client data.\n",
        "\n",
        "*Document Loading and Processing*: Documents are loaded from the Hugging Face platform, split into smaller sections by a LangChain Text Splitter, and pre-processed. The notes are split into smaller chunks, even though this probably was not necessary for this dataset. This step was taken for completeness to ensure scalability and efficiency as data complexity or volume increases in the future.  \n",
        "*Database Initialization and Population*: A Chroma vector database is initialized and populated it with the embedded document chunks.  \n",
        "*Query Operations*: Utilizes the RetrievalQA pipeline to search the database using natural language queries, demonstrating the application's capability to retrieve and display relevant information.\n",
        "\n",
        "### Goal\n",
        "My goal is to use this vector database to retrieve relevant examples for few-shot inference in order to create synthetic client records. This approach can help me improve the generation of this data by providing specific, contextually relevant examples that guide the model's results.\n",
        "\n",
        "### Setup and configuration\n",
        "- Mount Google Drive to persistently store the Chroma vector database.\n",
        "- Retrieve API keys for OpenAI and HuggingFace, providing authentication for accessing the [embedding model](https://platform.openai.com/docs/guides/embeddings), the [QA model](https://platform.openai.com/docs/models) and the [dataset](https://huggingface.co/datasets/ekrombouts/dutch_nursing_home_reports). ***Please note*** that the embedding isn't free. Embedding the 35.000+ notes costs appr $0.15. The costs for the examples of querying the database in this notebook are negligible.\n",
        "\n",
        "### Recommended Resources\n",
        "- [RAG - Retrieval Augmented Generation](https://www.youtube.com/playlist?list=PL8motc6AQftn-X1HkaGG9KjmKtWImCKJS) with Sam Witteveen on YouTube\n",
        "- [Python RAG Tutorial (with Local LLMs)](https://www.youtube.com/watch?v=2TJxpyO3ei4&t=323s) by Pixegami on YouTube\n",
        "- And of course the [Langchain documentation](https://python.langchain.com/v0.1/docs/use_cases/question_answering/)"
      ],
      "metadata": {
        "id": "y_63aAoRXj07"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ACPt1E5rHbj"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -q langchain langchain-openai langchain-community chromadb datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules from Langchain and Hugging Face\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from google.colab import drive, userdata\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
        "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.chains import RetrievalQA\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "nxPS7wYjrugf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants for dataset and storage paths\n",
        "PATH_HF_DATASET = 'ekrombouts/dutch_nursing_home_reports'\n",
        "PATH_DB_GCAI = '/content/drive/MyDrive/Colab Notebooks/GenCareAI/data/chroma_db_gcai'\n",
        "COLLECTION_NAME = 'anonymous_reports'\n",
        "MODEL = 'text-embedding-ada-002'"
      ],
      "metadata": {
        "id": "YCGSQTBMrzxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "tILbjl3zuQI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents():\n",
        "  \"\"\"Load the dataset from Hugging Face, ensuring access via token.\"\"\"\n",
        "  dataset = load_dataset(PATH_HF_DATASET, token=HF_TOKEN)\n",
        "  loader = HuggingFaceDatasetLoader(PATH_HF_DATASET,\n",
        "                                    page_content_column='report')\n",
        "  return loader.load()"
      ],
      "metadata": {
        "id": "l29SPKZNsfuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_documents(documents: list[Document]):\n",
        "  \"\"\"Split large text documents into manageable chunks for better handling by ML models.\"\"\"\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size=800,\n",
        "      chunk_overlap=100)\n",
        "  chunks = text_splitter.split_documents(documents)\n",
        "  # Index each chunk to maintain unique identifiers\n",
        "  for idx, chunk in enumerate(chunks):\n",
        "      chunk.metadata['id'] = str(idx)\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "QCRJwVFXuyE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_vectordb(persist_directory, embedding_function, collection_name):\n",
        "    \"\"\"Initialize the Chroma vector database, either loading an existing one or creating a new one.\"\"\"\n",
        "    if os.path.exists(persist_directory):\n",
        "        return Chroma(persist_directory=persist_directory,\n",
        "                      embedding_function=embedding_function,\n",
        "                      collection_name=collection_name)\n",
        "    else:\n",
        "        return Chroma.from_documents(documents=[],\n",
        "                                     embedding=embedding_function,\n",
        "                                     persist_directory=persist_directory,\n",
        "                                     collection_name=collection_name)"
      ],
      "metadata": {
        "id": "NZKYyBXp7iTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_existing_ids(vectordb):\n",
        "    \"\"\"Fetch existing document IDs from the database to avoid duplicates.\"\"\"\n",
        "    try:\n",
        "        existing_items = vectordb.get(include=[])\n",
        "        existing_ids = set(existing_items[\"ids\"])\n",
        "    except:\n",
        "        existing_ids = set()\n",
        "    return existing_ids"
      ],
      "metadata": {
        "id": "wild1x4IpGYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified from https://github.com/pixegami/rag-tutorial-v2/blob/main/populate_database.py\n",
        "def add_new_documents(vectordb, documents):\n",
        "    \"\"\"Add new documents to the database only if they don't already exist.\"\"\"\n",
        "    existing_ids = load_existing_ids(vectordb)\n",
        "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
        "    # Only add documents that don't exist in the DB.\n",
        "    new_documents = []\n",
        "    for document in documents:\n",
        "        if document.metadata[\"id\"] not in existing_ids:\n",
        "            new_documents.append(document)\n",
        "    if len(new_documents):\n",
        "        print(f\"Adding new documents: {len(new_documents)}\")\n",
        "        new_document_ids = [document.metadata[\"id\"] for document in new_documents]\n",
        "        vectordb.add_documents(new_documents, ids=new_document_ids)\n",
        "    else:\n",
        "        print(\"No new documents to add\")\n"
      ],
      "metadata": {
        "id": "4Psz5WVXpope"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embed and store texts"
      ],
      "metadata": {
        "id": "LK31hG7c1T6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup and authentication\n",
        "drive.mount('/content/drive')\n",
        "OPENAI_API_KEY = userdata.get('GCI_OPENAI_API_KEY')\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "bLWWHtDar4wO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load, split and process documents\n",
        "documents = load_documents()\n",
        "chunks = split_documents(documents=documents)\n",
        "\n",
        "# # Consider experimenting with a smaller dataset\n",
        "# documents_sample = documents[:15]\n",
        "# chunks = split_documents(documents=documents_sample)\n",
        "\n",
        "print(len(documents))\n",
        "print(len(chunks))"
      ],
      "metadata": {
        "id": "REruMCsLyNoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize vector databse, using OpenAI embeddings\n",
        "embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=MODEL)\n",
        "vectordb = initialize_vectordb(PATH_DB_GCAI, embedding, COLLECTION_NAME)"
      ],
      "metadata": {
        "id": "2MFHbqkV9eIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add new documents to the database\n",
        "add_new_documents(vectordb, chunks)"
      ],
      "metadata": {
        "id": "vrk3uLoB93g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query the database"
      ],
      "metadata": {
        "id": "56otpzQ509Lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## To read the db from file\n",
        "\n",
        "# vectordb = Chroma(persist_directory=FN_DB_GCAI,\n",
        "#                   embedding_function=OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=MODEL),\n",
        "#                   collection_name = COLLECTION_NAME\n",
        "#                   )"
      ],
      "metadata": {
        "id": "tuGkHJ26AhVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete all items in the db\n",
        "\n",
        "# items = vectordb.get(include=[])\n",
        "# existing_ids = items[\"ids\"]\n",
        "# vectordb.delete(ids=existing_ids)"
      ],
      "metadata": {
        "id": "WVUpJYDt2eNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve metadata and document IDs from the database\n",
        "items = vectordb.get(include=['metadatas'])\n",
        "existing_ids = set(items[\"ids\"])\n",
        "metadata = items['metadatas']\n",
        "print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
        "print(metadata[0])"
      ],
      "metadata": {
        "id": "cOkFn9g_-XSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up a retriever for document querying\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})"
      ],
      "metadata": {
        "id": "tEQfxXps_D0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the vector database using similarity search\n",
        "query = 'gewichtsverlies'\n",
        "docs = retriever.invoke(query)\n",
        "\n",
        "print(f'Number of docs: {len(docs)}\\n')\n",
        "print(f'Retriever search type: {retriever.search_type}\\n')\n",
        "\n",
        "print(f'Documents most similar to \"{query}\":')\n",
        "for doc in docs:\n",
        "  print(doc.page_content)"
      ],
      "metadata": {
        "id": "ZOLn6rsX-k7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the QA chain for answering questions using the retrieved documents\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(api_key=OPENAI_API_KEY),\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "hfgNejX3_WrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to process the response\n",
        "def process_llm_response(llm_response):\n",
        "    print(100 * '*')\n",
        "    print(f\"\\nresult: {llm_response['result']}\")\n",
        "    print('\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['id'], source.metadata['topic'])"
      ],
      "metadata": {
        "id": "VrJeQA1Y5Rec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to test the pipeline\n",
        "def query_retrieval_pipeline(query):\n",
        "  llm_response = qa_chain.invoke(query)\n",
        "  pprint(llm_response)\n",
        "  print(process_llm_response(llm_response))"
      ],
      "metadata": {
        "id": "vUHXH3SYCYbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_retrieval_pipeline (\"Wat moet je doen als je client afvalt in gewicht?\")"
      ],
      "metadata": {
        "id": "GD6uq1UzS8ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_retrieval_pipeline(\"Wat moet je doen als je client agressief gedrag vertoont?\")"
      ],
      "metadata": {
        "id": "eFZ8vW4-C-LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_retrieval_pipeline(\"Wat kan je doen als een cliÃ«nt onrustig is 's nachts?\")"
      ],
      "metadata": {
        "id": "0SJAb2aXDg0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_retrieval_pipeline(\"Welke interventies zijn ingezet voor het verbeteren van de nachtrust?\")"
      ],
      "metadata": {
        "id": "5_V8JtBEEZeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_retrieval_pipeline(\"Wat zijn leuke dingen om te doen met bezoek?\")"
      ],
      "metadata": {
        "id": "54V-v_euVl5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dwShUZWgn6Sx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}